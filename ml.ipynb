{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single value prediction\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "c=pd.read_csv('/Users/abhisheksingh/Downloads/canada_per_capita_income.csv')\n",
    "c.rename(columns={'per capita income (US$)':'income'},inplace=True)   \n",
    "c\n",
    "v=c.loc[[1,2,3,4,5],['year','income']]                  \n",
    "\n",
    "%matplotlib inline\n",
    "plt.xlabel('year')\n",
    "plt.ylabel(' income (US$)')\n",
    "plt.scatter(c.year,c.income)\n",
    "reg=linear_model.LinearRegression()\n",
    "reg.fit(v[['year']],v.income)\n",
    "# reg.predict([[2020]])\n",
    "#saving a file \n",
    "import joblib as jb\n",
    "jb.dump(reg,\"model_joblib\")\n",
    "kl=jb.load('model_joblib')\n",
    "kl.predict([[2020]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_descent cost function we can choose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def gradient_descent(x,y):\n",
    "    learning_rate=0.08\n",
    "    iterations= 1000\n",
    "    n=len(x)\n",
    "    m_c=b_c=0\n",
    "    for i in range (iterations):\n",
    "        y_pred=m_c*x + b_c\n",
    "        cost= (1/n)*sum(val**2 for val in (y-y_pred))\n",
    "        md= -(2/n)*sum(x*(y-y_pred))\n",
    "        bd= -(2/n)*sum((y-y_pred))\n",
    "        m_c=m_c - learning_rate*md\n",
    "        b_c=b_c - learning_rate*bd\n",
    "        # print('m{} b{} cost{}'.format(m_c,b_c,cost))\n",
    "        plt.plot(x,m_c*y+b_c)\n",
    "\n",
    "x=np.array([1,2,3,4,5])        \n",
    "y=np.array([5,7,9,11,13])\n",
    "gradient_descent(x,y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning the file\n",
    "# d['prices']=reg.predict\n",
    "# d.to_csv('filename.csv',index=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the predicted value line\n",
    "plt.xlabel('year')\n",
    "plt.ylabel(' income (US$)')\n",
    "plt.scatter(v.year,v.income)\n",
    "plt.plot(v.year,reg.predict(v[['year']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "c=pd.read_csv('/Users/abhisheksingh/Downloads/canada_per_capita_income.csv')\n",
    "c.rename(columns={'per capita income (US$)':'income'},inplace=True)   \n",
    "%matplotlib inline\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('income')\n",
    "plt.scatter(c.year,c.income)\n",
    "\n",
    "reg=linear_model.LinearRegression()\n",
    "reg.fit(c[['year']],c.income)\n",
    "reg.predict([[2020]])\n",
    "plt.plot(c.year,reg.predict(c[['year']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multivariable linear regression\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "r=pd.read_excel('/Users/abhisheksingh/Downloads/Book 5 (1).xlsx')\n",
    "median_bedrooms=int(r['bedrooms'].median())\n",
    "r.loc[2,'bedrooms']=median_bedrooms   # can also use r['bedrooms'].fillna(median_bedrooms)\n",
    "reg=linear_model.LinearRegression()\n",
    "r\n",
    "reg.fit(r[['Area ','bedrooms','age']],r.price)\n",
    "reg.coef_\n",
    "reg.intercept_\n",
    "y=int(reg.predict([[3000,3,40]]))\n",
    "r.loc[5,['Area ','bedrooms','age','price']]=[3000,3,40,y]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving a file \n",
    "import joblib as jb\n",
    "jb.dump(reg,\"model_joblib\")\n",
    "kl=jb.load('model_joblib')\n",
    "kl.predict([[2020]])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Dummy variables\n",
    "import pandas as pd\n",
    "c=pd.read_excel('/Users/abhisheksingh/Downloads/pp/Book 6.xlsx')\n",
    "dummies=pd.get_dummies(c,drop_first=True)  #impo\n",
    "dummies\n",
    "u=dummies.drop(['area','price'],axis=1)\n",
    "merged=pd.concat([c,u],axis=1)\n",
    "final=merged.drop(['town'],axis=1)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg=LinearRegression()\n",
    "X=final.drop('price',axis=1)\n",
    "Y=final.price\n",
    "reg.fit(X,Y)\n",
    "reg.predict([[2800,0,1,0]])\n",
    "reg.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rlabelencoding\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "l=LabelEncoder()\n",
    "c=pd.read_excel('/Users/abhisheksingh/Downloads/pp/Book 6.xlsx')\n",
    "hl=c\n",
    "hl.town=l.fit_transform(hl.town) # IMPO\n",
    "# x=hl[['town','area']].values\n",
    "# print(x)\n",
    "# y=hl.price\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe=OneHotEncoder(categorical_features=[0])\n",
    "# ohe.fit_transform(x).toarray()\n",
    "hl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies example\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline \n",
    "c=pd.read_csv('/Users/abhisheksingh/Downloads/pp/carprices.csv')\n",
    "p=pd.get_dummies(c)\n",
    "l=p.drop('Sell Price($)',axis=1)\n",
    "# o=pd.concat([c['Car Model'],p],axis=1)\n",
    "y=c['Sell Price($)']\n",
    "reg=LinearRegression()\n",
    "reg.fit(l,y)\n",
    "l\n",
    "reg.predict([[45000,4,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x=c[['mileage','age']]   # for training any model the input data must be in 2d form\n",
    "y=c['sell price']\n",
    "x_train,x_test,y_train,y_split=train_test_split(x,y,testsize=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "c=pd.read_csv('/Users/abhisheksingh/Downloads/pp/insurance_data.csv')\n",
    "# plt.scatter(c.age,c.bought_insurance,marker='*')\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(c[['age']],c['bought_insurance'],test_size=0.1,random_state=42)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "reg=LogisticRegression()\n",
    "reg.fit(xtrain,ytrain)\n",
    "print(xtest)\n",
    "reg.predict_proba(xtest)\n",
    "reg.predict([[25]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression multiclass classify\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "d=load_digits()\n",
    "dir(d)   #describes what attributes does a digit element have \n",
    "d.data[0]\n",
    "plt.gray()\n",
    "plt.matshow(d.images[0])\n",
    "d.target[0:5]  # traget/output\n",
    "# data and traget to train our model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(d.data,d.target,test_size=0.2,random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "reg=LogisticRegression()\n",
    "reg.fit(xtrain,ytrain)\n",
    "reg.score(xtest,ytest)\n",
    "plt.matshow(d.images[67])\n",
    "d.target[67]\n",
    "reg.predict([d.data[67]])\n",
    "reg.predict(d.data[0:5])\n",
    "\n",
    "#confusion matrix\n",
    "\n",
    "ypred=reg.predict(xtest)\n",
    "#using seaborn to better visualize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "import seaborn as sn\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(cm,annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression example\n",
    "import sklearn\n",
    "import pandas as  pd\n",
    "from sklearn.datasets import load_iris\n",
    "i=load_iris()\n",
    "i.target_names[1]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "f=LogisticRegression()\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(i.data,i.target,test_size=0.2,random_state=42)\n",
    "f.fit(xtrain,ytrain)\n",
    "f.score(xtest,ytest)\n",
    "sample=pd.read_csv(\"/Users/abhisheksingh/Downloads/iris_petal_sepal.csv\")\n",
    "f.predict(sample)\n",
    "\n",
    "\n",
    "# 'DESCR',\n",
    "#  'data',\n",
    "#  'data_module',\n",
    "#  'feature_names',\n",
    "#  'filename',\n",
    "#  'frame',\n",
    "#  'target',\n",
    "#  'target_names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descision tree          look for descision tree criteria\n",
    "import pandas as pd\n",
    "c=pd.read_csv('/Users/abhisheksingh/Downloads/pp/salaries.csv')\n",
    "c.head()\n",
    "input=c.drop('salary_more_then_100k',axis=1)\n",
    "target=c['salary_more_then_100k']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "l_company=LabelEncoder()\n",
    "input['company_n']=l_company.fit_transform(input['company'])\n",
    "input['job_n']=l_company.fit_transform(input['job'])\n",
    "input['degree_n']=l_company.fit_transform(input['degree'])\n",
    "input_n=input.drop(['company','job','degree'],axis=1) \n",
    "input_n\n",
    "# from sklearn import tree\n",
    "# m=tree.DecisionTreeClassifier()\n",
    "# m.fit(input_n,target)\n",
    "# m.score(input_n,target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descision tree example\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t=pd.read_csv(\"/Users/abhisheksingh/Downloads/pp/titanic.csv\")\n",
    "t.head()\n",
    "ti=t.drop(['PassengerId','Name','SibSp','Parch','Ticket','Embarked','Survived','Cabin'],axis=1)\n",
    "target=t['Survived']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "l=LabelEncoder()\n",
    "ti['sex_n']=l.fit_transform(ti['Sex'])\n",
    "titn=ti.drop('Sex',axis=1)\n",
    "tit=titn.dropna(subset=['Age'])\n",
    "pd.set_option('display.max.rows',89)\n",
    "tit\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(titn,target,test_size=0.2,random_state=42)\n",
    "m=tree.DecisionTreeClassifier()\n",
    "m.fit(xtrain,ytrain)\n",
    "m.score(xtest,ytest)\n",
    "\n",
    "ypred=m.predict(xtest)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "import seaborn as sn\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(cm,annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "import pandas as pd \n",
    "from sklearn.datasets import load_iris\n",
    "i=load_iris()\n",
    "l=pd.DataFrame(i.data,columns=i.feature_names)\n",
    "l['target']=i.target\n",
    "l\n",
    "# def flowen(t):\n",
    "#     return i.target_names[t]\n",
    "# l['flower_name']=l.target.apply(flowen)\n",
    "\n",
    "\n",
    "# #visualizing the data to check for working of svm\n",
    "# # df0=l[l['target']==0]\n",
    "# # df1=l[l['target']==1]\n",
    "# # df2=l[l['target']==2]\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # plt.scatter(df0['sepal length (cm)'],df0['sepal width (cm)'],color='blue')\n",
    "# # plt.scatter(df1['sepal length (cm)'],df1['sepal width (cm)'],color='green')\n",
    "# # plt.scatter(df2['sepal length (cm)'],df2['sepal width (cm)'],color='yellow')\n",
    "\n",
    "# d=l.drop(['target','flower_name'],axis=1)\n",
    "# target=l['target']\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# xtrain,xtest,ytrain,ytest=train_test_split(d,target,test_size=0.2,random_state=42)\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# sv=SVC()\n",
    "# sv.fit(xtrain,ytrain)\n",
    "# sv.score(xtest,ytest)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM example\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "d=load_digits()\n",
    "# dir(d)  #['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n",
    "target=pd.DataFrame(d.target)\n",
    "data=pd.DataFrame(d.data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(d.data,d.target,test_size=0.2)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "n=SVC()\n",
    "n.fit(xtrain,ytrain)\n",
    "n.score(xtest,ytest)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.datasets import load_digits\n",
    "d=load_digits()\n",
    "# for i in range (4):\n",
    "#     plt.matshow(d.images[i])\n",
    "# r=pd.DataFrame(d.data)\n",
    "r['target']=d.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(r.drop('target',axis=1),r.target,test_size=0.2)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "m=RandomForestClassifier()\n",
    "m.fit(xtrain,ytrain)\n",
    "m.score(xtest,ytest)\n",
    "\n",
    "ypred=m.predict(xtest)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "import seaborn as sn\n",
    "sn.heatmap(cm,annot=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forst example\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "i=load_iris()\n",
    "dir(i)  \n",
    "#'DESCR',\n",
    "#  'data',\n",
    "#  'data_module',\n",
    "#  'feature_names',\n",
    "#  'filename',\n",
    "#  'frame',\n",
    "#  'target',\n",
    "#  'target_names']\n",
    "ir=pd.DataFrame(i.data,columns=i.feature_names)\n",
    "target=pd.DataFrame(i.target)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "s=RandomForestClassifier(n_estimators=9999)\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(ir,target,test_size=0.2,random_state=42)\n",
    "s.fit(xtrain,ytrain)\n",
    "s.score(xtest,ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validationfrom \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "d=load_digits()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(d.data,d.target,test_size=0.3)\n",
    "\n",
    "lr=LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "lr.score(x_test,y_test)\n",
    "\n",
    "s=SVC()\n",
    "s.fit(x_train,y_train)\n",
    "s.score(x_test,y_test)\n",
    "\n",
    "r=RandomForestClassifier()\n",
    "r.fit(x_train,y_train)\n",
    "r.score(x_test,y_test)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=5)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "f=KFold(n_splits=3)\n",
    "\n",
    "for train,test in f.split([1,2,3,4,5,6,7,8,9]):\n",
    "    print (train,test)\n",
    "\n",
    "# scorelr=[]\n",
    "# scores=[]\n",
    "# scorer=[]\n",
    "\n",
    "# def getscore(m,xtr,xte,ytr,yte):\n",
    "#     m.fit(xtr,ytr)\n",
    "#     return m.score(xte,yte)\n",
    "\n",
    "\n",
    "# for train,test in f.split(d.data):\n",
    "#     x_train,x_test,y_train,y_test=d.data[train],d.data[test],d.target[train],d.target[test]\n",
    "#     scorelr.append(getscore(lr,x_train,x_test,y_train,y_test))\n",
    "#     scorer.append(getscore(r,x_train,x_test,y_train,y_test))\n",
    "#     scores.append(getscore(s,x_train,x_test,y_train,y_test))\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# cross_val_score(lr,d.data,d.target,cv=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation example\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lr=LogisticRegression()\n",
    "r=RandomForestClassifier()\n",
    "ir=load_iris()\n",
    "cross_val_score (lr,ir.data,ir.target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " UNSUPERVISED LEARNING \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmenas clustering   #minmaxscaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "\n",
    "d= pd.read_csv('/Users/abhisheksingh/Downloads/pp/income.csv')\n",
    "d\n",
    "# plt.scatter(d['Age'],d['Income($)'])\n",
    "\n",
    "# km=KMeans(n_clusters=3)\n",
    "# y_pred=km.fit_predict(d[['Age','Income($)']])\n",
    "# d['cluster']=y_pred\n",
    "# df0=d[d['cluster']==0]\n",
    "# df1=d[d['cluster']==1]\n",
    "# df2=d[d['cluster']==2]\n",
    "# plt.scatter(df0['Age'],df0['Income($)'],color='red')\n",
    "# plt.scatter(df1['Age'],df1['Income($)'],color='blue')\n",
    "# plt.scatter(df2['Age'],df2['Income($)'],color='yellow')\n",
    "\n",
    "#scaling to get accurate clustring\n",
    "scaler=MinMaxScaler()\n",
    "d['Income($)']=scaler.fit_transform(d[['Income($)']])\n",
    "d['Age']=scaler.fit_transform(d[['Age']])\n",
    "d\n",
    "\n",
    "km=KMeans(n_clusters=3)\n",
    "y_pred=km.fit_predict(d[['Age','Income($)']])\n",
    "d['cluster']=y_pred\n",
    "df0=d[d['cluster']==0]\n",
    "df1=d[d['cluster']==1]\n",
    "df2=d[d['cluster']==2]\n",
    "km.cluster_centers_   #gives centres for each cluster\n",
    "plt.scatter(df0['Age'],df0['Income($)'],color='red')\n",
    "plt.scatter(df1['Age'],df1['Income($)'],color='blue')\n",
    "plt.scatter(df2['Age'],df2['Income($)'],color='yellow')\n",
    "plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='cyan',marker='*')\n",
    "\n",
    "k_r= range(1,11)\n",
    "sse=[]\n",
    "for k in k_r:\n",
    "    km=KMeans(n_clusters=k)\n",
    "    km.fit(d[['Age','Income($)']])\n",
    "    sse.append(km.inertia_)\n",
    "sse    \n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('sse')\n",
    "plt.plot(k_r,sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k cluster example\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "%matplotlib inline\n",
    "\n",
    "# pd.set_option('display.max.rows',160)\n",
    "ir=load_iris()\n",
    "dir(ir)\n",
    "l=pd.DataFrame(ir.data,columns=ir.feature_names)\n",
    "lp=l.drop(['sepal length (cm)','sepal width (cm)'],axis=1)\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "lp['petal length (cm)']=scaler.fit_transform(lp[['petal length (cm)']])\n",
    "lp['petal width (cm)']=scaler.fit_transform(lp[['petal width (cm)']])\n",
    "\n",
    "\n",
    "km=KMeans(n_clusters=3)\n",
    "y_pred=km.fit_predict(lp[['petal length (cm)','petal width (cm)']])\n",
    "lp['cluster']=y_pred\n",
    "\n",
    "df0=lp[lp['cluster']==0]\n",
    "df1=lp[lp['cluster']==1]\n",
    "df2=lp[lp['cluster']==2]\n",
    "\n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('petal width (cm)')\n",
    "plt.scatter(df0['petal length (cm)'],df0['petal width (cm)'],color='yellow')\n",
    "plt.scatter(df1['petal length (cm)'],df1['petal width (cm)'],color='blue')\n",
    "plt.scatter(df2['petal length (cm)'],df2['petal width (cm)'],color='green')\n",
    "km.cluster_centers_\n",
    "\n",
    "k_r=range(1,10)\n",
    "sse=[]\n",
    "for k in (k_r):\n",
    "    km=KMeans(n_clusters=k)\n",
    "    km.fit(lp[['petal length (cm)','petal width (cm)']])\n",
    "    sse.append(km.inertia_)\n",
    "sse    \n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('sse')\n",
    "plt.plot(sse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussain byes\n",
    "\n",
    "import pandas as  pd\n",
    "t=pd.read_csv('/Users/abhisheksingh/Downloads/pp/titanic.csv')\n",
    "t.head()\n",
    "input=t.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked','Survived','Sex'],axis=1)\n",
    "target=t['Survived']\n",
    "target\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le=LabelEncoder()\n",
    "# input['male']=le.fit_transform(input['Sex'])\n",
    "# input['female']=le.fit_transform(input['Sex'])\n",
    "dummies=pd.get_dummies(t['Sex'])\n",
    "dummies\n",
    "input['female']=dummies.female\n",
    "input['male']=dummies.male\n",
    "input.columns[input.isna().any()]  #checking for any na value\n",
    "\n",
    "# filling the na value mean of ages\n",
    "input['Age']=input['Age'].fillna(int(input['Age'].mean()))\n",
    "input.head(10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(input,target,test_size=0.2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(x_train,y_train)\n",
    "nb.score(x_test,y_test)\n",
    "print(y_test.head(10))\n",
    "print(nb.predict(x_test[:10]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial byes\n",
    "\n",
    "import pandas as pd\n",
    "l=pd.read_csv('/Users/abhisheksingh/Downloads/pp/spam.csv')\n",
    "\n",
    "def spam1(s):\n",
    "    if s=='spam' :\n",
    "        return 1\n",
    "    else: return 0\n",
    "l['spam']=l['Category'].apply(spam1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(l.Message,l.spam,test_size=0.2)\n",
    "\n",
    "# vectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v=CountVectorizer()\n",
    "x_train_counter=v.fit_transform(x_train)  #messages converted into array\n",
    "x_train_counter.toarray()\n",
    "# v.get_feature_names_out()[1:9]   # gives name of unique features \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mn=MultinomialNB()\n",
    "mn.fit(x_train_counter,y_train)      \n",
    "\n",
    "emails = [\n",
    "    'Hey mohan, can we get together to watch footbal game tomorrow?',\n",
    "    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'\n",
    "]\n",
    "email_counts=v.transform(emails)\n",
    "mn.predict(email_counts)\n",
    "\n",
    "x_test_count=v.transform(x_test)\n",
    "mn.score(x_test_count,y_test)\n",
    "\n",
    "# simplifying through pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "g=Pipeline([('v',CountVectorizer()),('mn',MultinomialNB())])\n",
    "g.fit(x_train,y_train)\n",
    "g.score(x_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes exercise\n",
    "# ['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']\n",
    "from sklearn.datasets import load_wine\n",
    "w=load_wine()\n",
    "import pandas as pd\n",
    "n=pd.DataFrame(w.data,columns=w.feature_names)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "m=GaussianNB()\n",
    "target=w.target\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_predict(m,n,target)\n",
    "cross_val_score(m,n,target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch and randomsearch\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "l=load_iris()\n",
    "k=['rbf','linear']\n",
    "c=[1,10,20]\n",
    "asv={}\n",
    "for i in c:\n",
    "    for j in k:\n",
    "        score=cross_val_score(SVC(kernel=j,C=i,gamma='auto'),l.data,l.target,cv=5)\n",
    "        asv[j+'_'+str(i)]=np.average(score)\n",
    "asv        \n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "y=GridSearchCV(SVC(gamma='auto'),{'C':[1,10,20],'kernel':['rbf','linear']},cv=5,return_train_score=False)\n",
    "y.fit(l.data,l.target)\n",
    "o=pd.DataFrame(y.cv_results_)\n",
    "o\n",
    "y.best_params_\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "p=RandomizedSearchCV(SVC(gamma='auto'),{'C':[1,10,20],'kernel':['rbf','linear']},cv=5,return_train_score=False,n_iter=2)  #n_iter dufferent combinations of given parameters \n",
    "p.fit(l.data,l.target)\n",
    "o=pd.DataFrame(p.cv_results_)\n",
    "\n",
    "\n",
    "model_parama={'svm':{'model':SVC(gamma='auto'),'params':{'C':[1,10,20]}},'random_forest':{'model':RandomForestClassifier(),'params':{'n_estimators':[1,5,10]}}}\n",
    "score=[]\n",
    "for model_name,mp in model_parama.items():\n",
    "    clf=GridSearchCV(mp['model'],mp['params'],cv=5,return_train_score=False)\n",
    "    clf.fit(l.data,l.target)\n",
    "    score.append({'model':model_name,'bestscore':clf.best_score_,'bestparams':clf.best_params_})\n",
    "score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "l=load_digits()\n",
    "\n",
    "models={'svm':{'model':SVC(gamma='auto'),'para':{'C':[1,10,20]}},'randomforest':{'model':RandomForestClassifier(),'para':{'n_estimators':[1,5,7],'criterion':['gini', 'entropy', 'log_loss']}},'logisticregression':{'model':LogisticRegression(solver='liblinear',multi_class=\"auto\"),'para':{'penalty':['l1', 'l2', 'elasticnet']}},'GAussianNb':{'model':GaussianNB(),'para':{}},'multinomialnb':{'model':MultinomialNB(),'para':{}},'decisiontree':{'model':DecisionTreeClassifier(),'para':{'criterion':[\"gini\", \"entropy\", \"log_loss\"]}}}\n",
    "\n",
    "score=[]\n",
    "for modelname,mp in models.items():\n",
    "    clf=GridSearchCV(mp['model'],mp['para'],cv=5,return_train_score=False)\n",
    "    clf.fit(l.data,l.target)\n",
    "    score.append({'model':modelname,'bestscore':clf.best_score_,'bestpara':clf.best_params_})\n",
    "m=pd.DataFrame(score,columns=['model','bestscore','bestpara'])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1,L2 reguralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "d=pd.read_csv('/Users/abhisheksingh/Downloads/pp/Melbourne_housing_FULL.csv')\n",
    "cols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount', \n",
    "               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']\n",
    "d=d[cols_to_use]\n",
    "\n",
    "d.isna().sum()\n",
    "cols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']\n",
    "d[cols_to_fill_zero]=d[cols_to_fill_zero].fillna(0)\n",
    "\n",
    "d['Landsize']=d['Landsize'].fillna(d['Landsize'].mean)\n",
    "d['BuildingArea']=d['BuildingArea'].fillna(d['BuildingArea'].mean)\n",
    "d.isna().sum()\n",
    "d.dropna(inplace=True)\n",
    "d.isna().sum()\n",
    "d=pd.get_dummies(d,drop_first=True)\n",
    "x=d.drop('Price',axis=1)\n",
    "y=d['Price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.3, random_state=2)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg=LinearRegression().fit(train_X,train_y)\n",
    "reg.score(train_X,train_y)\n",
    "\n",
    "#L1\n",
    "from sklearn import linear_model\n",
    "lr=linear_model.Lasso(alpha=50,max_iter=100,tol=0.1)\n",
    "lr.fit(train_X,train_y)\n",
    "lr.score(train_X,train_y)\n",
    "\n",
    "#L2\n",
    "from sklearn.linear_model import Ridge\n",
    "k=Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "i=load_iris()\n",
    "d=pd.DataFrame(i.data,columns=i.feature_names)\n",
    "d['target']=i.target\n",
    "df0=d[:50]\n",
    "df1=d[50:100]\n",
    "df2=d[100:150]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(df0['sepal length (cm)'],df0['sepal width (cm)'],color='green')\n",
    "plt.scatter(df1['sepal length (cm)'],df1['sepal width (cm)'],color='yellow')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = d.drop(['target'], axis='columns')\n",
    "y = d.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# KNN classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train,y_train)\n",
    "knn.score(x_test,y_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "ypred=knn.predict(x_test)\n",
    "cm=confusion_matrix(y_test,ypred)\n",
    "import seaborn as sn\n",
    "sn.heatmap(cm,annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('truth')\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "d=load_digits()\n",
    "input=d.data\n",
    "target=d.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=1)\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train,y_train)\n",
    "knn.score(x_test,y_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred=knn.predict(x_test)\n",
    "c=confusion_matrix(y_test,y_pred)\n",
    "sn.heatmap(c,annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (principal component anlysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "d=load_digits()\n",
    "d.data[0].reshape(8,8)\n",
    "\n",
    "#printing images using dataset\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.gray()\n",
    "plt.matshow(d.data[0].reshape(8,8))\n",
    "\n",
    "#getting unique values from target\n",
    "import numpy as np\n",
    "np.unique(d.target)\n",
    "\n",
    "df=pd.DataFrame(d.data,columns=d.feature_names)\n",
    "target=d.target\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "xscaled=scaler.fit_transform(df)\n",
    "xscaled\n",
    "\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# # x_train, x_test, y_train, y_test = train_test_split(xscaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# # from sklearn.linear_model import LogisticRegression\n",
    "# # lt=LogisticRegression()\n",
    "# # lt.fit(x_train,y_train)\n",
    "# # lt.score(x_test,y_test)\n",
    "\n",
    "# #pca\n",
    "# from sklearn.decomposition import PCA\n",
    "# p=PCA(n_components=2)\n",
    "# xpca=p.fit_transform(df)\n",
    "\n",
    "# p.explained_variance_ratio_  # for each pixel give the percent of useful info it contains\n",
    "# p.n_components  # numner of new columns\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(xpca, target, test_size=0.2, random_state=42)\n",
    "# lt=LogisticRegression(max_iter=1000)   #gradient descent can converge\n",
    "# lt.fit(x_train,y_train)\n",
    "# lt.score(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "d=pd.read_csv('/Users/abhisheksingh/Downloads/pp/heart.csv')\n",
    "d[d['Cholesterol']>(d.Cholesterol.mean()+3*d.Cholesterol.std())]\n",
    "d=d[d['Cholesterol']<=(d.Cholesterol.mean()+3*d.Cholesterol.std())]\n",
    "d=d[d['Oldpeak']<=(d.Oldpeak.mean()+3*d.Oldpeak.std())]\n",
    "d=d[d['RestingBP']<=(d.RestingBP.mean()+3*d.RestingBP.std())]\n",
    "d['Sex']=label.fit_transform(d['Sex'])\n",
    "d['RestingECG']=label.fit_transform(d['RestingECG'])\n",
    "d['ST_Slope']=label.fit_transform(d['ST_Slope'])\n",
    "d['RestingECG']=label.fit_transform(d['RestingECG'])\n",
    "d['ExerciseAngina']=label.fit_transform(d['ExerciseAngina'])\n",
    "d['ChestPainType']=label.fit_transform(d['ChestPainType'])\n",
    "\n",
    "input=d.drop('HeartDisease',axis=1)\n",
    "target=d.HeartDisease\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s=StandardScaler()\n",
    "input1=s.fit_transform(input)\n",
    "input1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(input1,target,test_size=0.2)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "p=PCA(0.95)\n",
    "xpca=p.fit_transform(input1)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(xpca,target,test_size=0.2)\n",
    "r=RandomForestClassifier()\n",
    "r.fit(x_train,y_train)\n",
    "r.score(x_test,y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENSEMBLE LEARNING : train multiple models on same dataset and then combine the results to get final output \n",
    "\n",
    "In random forest we sample both rows and columns \n",
    "\n",
    "Bagging: underlying model can be any\n",
    "Bagged trees : underlying model is a tree\n",
    "\n",
    "look for conditions to see where bagging works best at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "d=pd.read_csv('/Users/abhisheksingh/Downloads/pp/diabetes.csv')\n",
    "x=d.drop('Outcome',axis=1)\n",
    "y=d.Outcome\n",
    "d.describe()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s=StandardScaler()\n",
    "xc=s.fit_transform(x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test= train_test_split(xc,y,test_size=0.2,stratify=y,random_state=3)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "f=cross_val_score(DecisionTreeClassifier(),x,y,cv=4)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "b=BaggingClassifier(estimator=DecisionTreeClassifier(),n_estimators=100,max_samples=0.8,oob_score=True,random_state=99)\n",
    "\n",
    "r=cross_val_score(b,x,y,cv=4)\n",
    "r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=pd.read_csv('/Users/abhisheksingh/Downloads/pp/heart.csv')\n",
    "r[r['RestingBP']>r['RestingBP'].mean()+3*r['RestingBP'].std()]\n",
    "r=r[r['RestingBP']<=r['RestingBP'].mean()+3*r['RestingBP'].std()]   #Cholesterol\tMaxHR Oldpeak\n",
    "r[r['Cholesterol']>r['Cholesterol'].mean()+3*r['Cholesterol'].std()]\n",
    "r=r[r['Cholesterol']<=r['Cholesterol'].mean()+3*r['Cholesterol'].std()]\n",
    "r[r['Oldpeak']>r['Oldpeak'].mean()+3*r['Oldpeak'].std()]\n",
    "r=r[r['Oldpeak']<=r['Oldpeak'].mean()+3*r['Oldpeak'].std()]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "l=LabelEncoder()\n",
    "r['Sex']=l.fit_transform(r['Sex'])\n",
    "r['ChestPainType']=l.fit_transform(r['ChestPainType'])\n",
    "r['RestingECG']=l.fit_transform(r['RestingECG'])\n",
    "r['ExerciseAngina']=l.fit_transform(r['ExerciseAngina'])\n",
    "r['ST_Slope']=l.fit_transform(r['ST_Slope'])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x=r.drop('HeartDisease',axis=1)\n",
    "xscaled=StandardScaler().fit_transform(x)\n",
    "y=r['HeartDisease']\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "s=SVC()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "o=cross_val_score(s,xscaled,y,cv=5)\n",
    "o.mean()  #0.8258809085328422\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "b=BaggingClassifier(estimator=SVC(),max_samples=0.8,n_estimators=100,oob_score=True,random_state=99)\n",
    "\n",
    "e=cross_val_score(b,xscaled,y,cv=5)\n",
    "e.mean()  #0.8269981583793738\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "q=DecisionTreeClassifier()\n",
    "\n",
    "w=cross_val_score(q,xscaled,y,cv=5)\n",
    "w.mean()  #0.7449048496009822\n",
    "\n",
    "b=BaggingClassifier(estimator=DecisionTreeClassifier(),max_samples=0.8,n_estimators=100,oob_score=True,random_state=99)\n",
    "\n",
    "\n",
    "w=cross_val_score(b,xscaled,y,cv=5)\n",
    "w.mean()  #0.8103437691835481\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d=pd.read_csv('/Users/abhisheksingh/Downloads/train.csv')\n",
    "d.head()\n",
    "s=pd.read_csv('/Users/abhisheksingh/Downloads/sampleSubmission.csv')\n",
    "d.head()\n",
    "d1=d.drop(['Descript','Resolution'],axis=1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "la=LabelEncoder()\n",
    "d1.Dates=la.fit_transform(d1.Dates)\n",
    "d1.Category=la.fit_transform(d1.Category)\n",
    "d1.DayOfWeek=la.fit_transform(d1.DayOfWeek)\n",
    "d1.PdDistrict=la.fit_transform(d1.PdDistrict)\n",
    "d1.Address=la.fit_transform(d1.Address)\n",
    "\n",
    "input=d1.drop('Category',axis=1)\n",
    "target=d1.Category\n",
    "\n",
    "d1.head()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "s=RandomForestClassifier()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(input,target,test_size=0.2)\n",
    "\n",
    "s.fit(x_train,y_train)\n",
    "s.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time step series\n",
    "# dp['time']=np.arange(len(dp.index))\n",
    "# dp['lag_1]=dp['target'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trends\n",
    "# The trend component of a time series represents a persistent, long-term change in the mean of the series. The trend is the slowest-moving part of a series, the part representing the largest time scale of importance.\n",
    "# More generally though, any persistent and slow-moving change in a series could constitute a trend -- time series commonly have trends in their variation for instance.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tunnel=pd.read_csv('/Users/abhisheksingh/Downloads/tunnel.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tunnel = tunnel.set_index(\"Day\")\n",
    "tunnel.head()\n",
    "# moving_average = tunnel.rolling(window=365,center=True,min_periods=183).mean()  \n",
    "# ax = tunnel.plot(style=\".\", color=\"0.5\")\n",
    "# moving_average.plot(ax=ax, linewidth=3, title=\"Tunnel Traffic - 365-Day Moving Average\", legend=False)\n",
    "\n",
    "moving_avergae=tunnel.rolling(window=365,center=True,min_periods=183).mean()\n",
    "ax=tunnel.plot(style='.')\n",
    "moving_avergae.plot(ax=ax,linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index=tunnel.index,  # dates from the training data\n",
    "    constant=True,       # dummy feature for the bias (y_intercept)\n",
    "    order=1,             # the time dummy (trend)\n",
    "    drop=True,           # drop terms if necessary to avoid collinearity\n",
    ")\n",
    "# `in_sample` creates features for the dates given in the `index` argument\n",
    "# X = dp.in_sample()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "y=tunnel['NumVehicles']\n",
    "model=LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = pd.Series(model.predict(X), index=X.index)\n",
    "y=tunnel.plot(style='.')\n",
    "y=y_pred.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "tunnel=pd.read_csv('/Users/abhisheksingh/Downloads/tunnel.csv',parse_dates=True) \n",
    "from sklearn.linear_model import LinearRegression\n",
    "tunnel=tunnel.set_index('Day')\n",
    "tunnel\n",
    "\n",
    "moving_avergae=tunnel.rolling(window=365,center=True,min_periods=183).mean()\n",
    "ax=tunnel.plot(style='.')\n",
    "moving_avergae.plot(ax=ax,linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "dp=DeterministicProcess(index=tunnel.index,order=1,constant=True,drop=True)\n",
    "x=dp.in_sample()\n",
    "y=tunnel['NumVehicles']\n",
    "model=LinearRegression()\n",
    "model.fit(x, y)\n",
    "y_pred = pd.Series(model.predict(x), index=x.index)\n",
    "y=tunnel.plot(style='.')\n",
    "y=y_pred.plot()\n",
    "\n",
    "#forecasting \n",
    "X_fore = dp.out_of_sample(steps=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs\n",
    "# ax = y.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\n",
    "# ax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\", color='C0')\n",
    "# ax = y_fore.plot(ax=ax, linewidth=3, label=\"Trend Forecast\", color='C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Seasonality :\n",
    "  The first kind, indicators, is best for a season with few observations, like a weekly season of daily observations:\n",
    "     Seasonal indicators are what you get if you treat a seasonal period as a categorical feature and apply one-hot encoding.\n",
    "\n",
    "\n",
    "  The second kind, Fourier features, is best for a season with many observations, like an annual season of daily observations:\n",
    "     If we add a set of these sine / cosine curves to our training data, the linear regression algorithm will figure out the weights that will fit the seasonal component in the target series.\n",
    "\n",
    "     we only needed eight features (four sine / cosine pairs) to get a good estimate of the annual seasonality.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
